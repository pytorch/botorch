{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE MNIST example: BO in a latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we use the MNIST dataset and some standard PyTorch examples to show a synthetic problem where the input to the objective function is a `28 x 28` image. The main idea is to train a [variational auto-encoder (VAE)](https://arxiv.org/abs/1312.6114) on the MNIST dataset and run Bayesian Optimization in the latent space. We also refer readers to [this tutorial](http://krasserm.github.io/2018/04/07/latent-space-optimization/), which discusses [the method](https://arxiv.org/abs/1610.02415) of jointly training a VAE with a predictor (e.g., classifier), and shows a similar tutorial for the MNIST setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets # transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.double\n",
    "SMOKE_TEST = os.environ.get(\"CI\", \"false\") == \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem setup\n",
    "\n",
    "Let's first define our synthetic expensive-to-evaluate objective function. We assume that it takes the following form:\n",
    "\n",
    "$$\\text{image} \\longrightarrow \\text{image classifier} \\longrightarrow \\text{scoring function} \n",
    "\\longrightarrow \\text{score}.$$\n",
    "\n",
    "The classifier is a convolutional neural network (CNN) trained using the architecture of the [PyTorch CNN example](https://github.com/pytorch/examples/tree/master/mnist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next instantiate the CNN for digit recognition and load a pre-trained model.\n",
    "\n",
    "Here, you may have to change `PRETRAINED_LOCATION` to the location of the `pretrained_models` folder on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_LOCATION = \"./pretrained_models\"\n",
    "\n",
    "cnn_model = Net().to(dtype=dtype, device=device)\n",
    "cnn_state_dict = torch.load(os.path.join(PRETRAINED_LOCATION, \"mnist_cnn.pt\"), map_location=device)\n",
    "cnn_model.load_state_dict(cnn_state_dict);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our VAE model follows the [PyTorch VAE example](https://github.com/pytorch/examples/tree/master/vae), except that we use the same data transform from the CNN tutorial for consistency. We then instantiate the model and again load a pre-trained model. To train these models, we refer readers to the PyTorch Github repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "    \n",
    "vae_model = VAE().to(dtype=dtype, device=device)\n",
    "vae_state_dict = torch.load(os.path.join(PRETRAINED_LOCATION, \"mnist_vae.pt\"), map_location=device)\n",
    "vae_model.load_state_dict(vae_state_dict);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the scoring function that maps digits to scores. The function below prefers the digit '3'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y):\n",
    "    \"\"\"Returns a 'score' for each digit from 0 to 9. It is modeled as a squared exponential\n",
    "    centered at the digit '3'.\n",
    "    \"\"\"\n",
    "    return torch.exp(-2 * (y - 3)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the scoring function, we can now write our overall objective, which as discussed above, starts with an image and outputs a score. Let's say the objective computes the expected score given the probabilities from the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_image_recognition(x):\n",
    "    \"\"\"The input x is an image and an expected score based on the CNN classifier and\n",
    "    the scoring function is returned.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        probs = torch.exp(cnn_model(x))  # b x 10\n",
    "        scores = score(torch.arange(10, device=device, dtype=dtype)).expand(probs.shape)\n",
    "    return (probs * scores).sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a helper function `decode` that takes as input the parameters `mu` and `logvar` of the variational distribution and performs reparameterization and the decoding. We use batched Bayesian optimization to search over the parameters `mu` and `logvar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(train_x):\n",
    "    with torch.no_grad():\n",
    "        decoded = vae_model.decode(train_x)\n",
    "    return decoded.view(train_x.shape[0], 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model initialization and initial random batch\n",
    "\n",
    "We use a `SingleTaskGP` to model the score of an image generated by a latent representation. The model is initialized with points drawn from $[-6, 6]^{20}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
    "from botorch.utils.transforms import standardize, normalize, unnormalize\n",
    "\n",
    "d = 20\n",
    "bounds = torch.tensor([[-6.0] * d, [6.0] * d], device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "def gen_initial_data(n=5):\n",
    "    # generate training data  \n",
    "    train_x = unnormalize(torch.rand(n, d, device=device, dtype=dtype), bounds=bounds)\n",
    "    train_obj = score_image_recognition(decode(train_x)).unsqueeze(-1)  # add output dimension\n",
    "    best_observed_value = train_obj.max().item()\n",
    "    return train_x, train_obj, best_observed_value\n",
    "\n",
    "\n",
    "def get_fitted_model(train_x, train_obj, state_dict=None):\n",
    "    # initialize and fit model\n",
    "    model = SingleTaskGP(train_X=train_x, train_Y=train_obj)\n",
    "    if state_dict is not None:\n",
    "        model.load_state_dict(state_dict)\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    mll.to(train_x)\n",
    "    fit_gpytorch_model(mll)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a helper function that performs the essential BO step\n",
    "The helper function below takes an acquisition function as an argument, optimizes it, and returns the batch $\\{x_1, x_2, \\ldots x_q\\}$ along with the observed function values. For this example, we'll use a small batch of $q=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "\n",
    "BATCH_SIZE = 3 if not SMOKE_TEST else 2\n",
    "NUM_RESTARTS = 10 if not SMOKE_TEST else 2\n",
    "RAW_SAMPLES = 256 if not SMOKE_TEST else 4\n",
    "\n",
    "\n",
    "def optimize_acqf_and_get_observation(acq_func):\n",
    "    \"\"\"Optimizes the acquisition function, and returns a new candidate and a noisy observation\"\"\"\n",
    "    \n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=torch.stack([\n",
    "            torch.zeros(d, dtype=dtype, device=device), \n",
    "            torch.ones(d, dtype=dtype, device=device),\n",
    "        ]),\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,\n",
    "    )\n",
    "\n",
    "    # observe new values \n",
    "    new_x = unnormalize(candidates.detach(), bounds=bounds)\n",
    "    new_obj = score_image_recognition(decode(new_x)).unsqueeze(-1)  # add output dimension\n",
    "    return new_x, new_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Bayesian Optimization loop with qEI\n",
    "The Bayesian optimization \"loop\" for a batch size of $q$ simply iterates the following steps: (1) given a surrogate model, choose a batch of points $\\{x_1, x_2, \\ldots x_q\\}$, (2) observe $f(x)$ for each $x$ in the batch, and (3) update the surrogate model. We run `N_BATCH=75` iterations. The acquisition function is approximated using `MC_SAMPLES=2048` samples. We also initialize the model with 5 randomly drawn points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch import fit_gpytorch_model\n",
    "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
    "from botorch.sampling.samplers import SobolQMCNormalSampler\n",
    "\n",
    "seed=1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "N_BATCH = 50 if not SMOKE_TEST else 3\n",
    "MC_SAMPLES = 2048 if not SMOKE_TEST else 32\n",
    "best_observed = []\n",
    "\n",
    "# call helper function to initialize model\n",
    "train_x, train_obj, best_value = gen_initial_data(n=5)\n",
    "best_observed.append(best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run the BO loop (this make take a few minutes, depending on your machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running BO .................................................."
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "print(f\"\\nRunning BO \", end='')\n",
    "\n",
    "state_dict = None\n",
    "# run N_BATCH rounds of BayesOpt after the initial random batch\n",
    "for iteration in range(N_BATCH):    \n",
    "\n",
    "    # fit the model\n",
    "    model = get_fitted_model(\n",
    "        normalize(train_x, bounds=bounds), \n",
    "        standardize(train_obj), \n",
    "        state_dict=state_dict,\n",
    "    )\n",
    "    \n",
    "    # define the qNEI acquisition module using a QMC sampler\n",
    "    qmc_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES, seed=seed)\n",
    "    qEI = qExpectedImprovement(model=model, sampler=qmc_sampler, best_f=standardize(train_obj).max())\n",
    "\n",
    "    # optimize and get new observation\n",
    "    new_x, new_obj = optimize_acqf_and_get_observation(qEI)\n",
    "\n",
    "    # update training points\n",
    "    train_x = torch.cat((train_x, new_x))\n",
    "    train_obj = torch.cat((train_obj, new_obj))\n",
    "\n",
    "    # update progress\n",
    "    best_value = train_obj.max().item()\n",
    "    best_observed.append(best_value)\n",
    "    \n",
    "    state_dict = model.state_dict()\n",
    "    \n",
    "    print(\".\", end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EI recommends the best point observed so far. We can visualize what the images corresponding to recommended points *would have* been if the BO process ended at various times. Here, we show the progress of the algorithm by examining the images at 0%, 10%, 25%, 50%, 75%, and 100% completion. The first image is the best image found through the initial random batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x1008 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 6, figsize=(14, 14))\n",
    "percentages = np.array([0, 10, 25, 50, 75, 100], dtype=np.float32)\n",
    "inds = (N_BATCH * BATCH_SIZE * percentages / 100 + 4).astype(int)\n",
    "\n",
    "for i, ax in enumerate(ax.flat):\n",
    "    b = torch.argmax(score_image_recognition(decode(train_x[:inds[i],:])), dim=0)\n",
    "    img = decode(train_x[b].view(1, -1)).squeeze().cpu()\n",
    "    ax.imshow(img, alpha=0.8, cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
