<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Acquisition Functions · BoTorch</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="&lt;p&gt;Acquisition functions are heuristics employed to evaluate the usefulness of one&lt;/p&gt;
"/><meta name="docsearch:version" content="0.1.0"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Acquisition Functions · BoTorch"/><meta property="og:type" content="website"/><meta property="og:url" content="https://botorch.org/v/0.1.0/"/><meta property="og:description" content="&lt;p&gt;Acquisition functions are heuristics employed to evaluate the usefulness of one&lt;/p&gt;
"/><meta property="og:image" content="https://botorch.org/v/0.1.0/img/botorch.png"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://botorch.org/v/0.1.0/img/botorch.png"/><link rel="shortcut icon" href="/v/0.1.0/img/botorch.ico"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><script>
              (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
              (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
              m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
              })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

              ga('create', 'UA-139570076-2', 'auto');
              ga('send', 'pageview');
            </script><link rel="stylesheet" href="/v/0.1.0/css/code_block_buttons.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="/v/0.1.0/js/code_block_buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/v/0.1.0/js/mathjax.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML"></script><script src="https://unpkg.com/vanilla-back-to-top@7.1.14/dist/vanilla-back-to-top.min.js"></script><script>
        document.addEventListener('DOMContentLoaded', function() {
          addBackToTop(
            {"zIndex":100}
          )
        });
        </script><script src="/v/0.1.0/js/scrollSpy.js"></script><link rel="stylesheet" href="/v/0.1.0/css/main.css"/><script src="/v/0.1.0/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/v/0.1.0/"><img class="logo" src="/v/0.1.0/img/botorch_logo_lockup_white.png" alt="BoTorch"/><h2 class="headerTitleWithLogo">BoTorch</h2></a><a href="/v/0.1.0/versions"><h3>0.1.0</h3></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class="siteNavGroupActive"><a href="/v/0.1.0/docs/introduction" target="_self">Docs</a></li><li class=""><a href="/v/0.1.0/tutorials/" target="_self">Tutorials</a></li><li class=""><a href="/v/0.1.0/api/" target="_self">API Reference</a></li><li class=""><a href="https://github.com/pytorch/botorch" target="_self">GitHub</a></li><li class=""><a target="_self"></a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="container docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>Basic Concepts</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">About</h3><ul class=""><li class="navListItem"><a class="navItem" href="/v/0.1.0/docs/introduction">Introduction</a></li><li class="navListItem"><a class="navItem" href="/v/0.1.0/docs/design_philosophy">Design Philosophy</a></li><li class="navListItem"><a class="navItem" href="/v/0.1.0/docs/botorch_and_ax">Using BoTorch with Ax</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">General</h3><ul class=""><li class="navListItem"><a class="navItem" href="/v/0.1.0/docs/getting_started">Getting Started</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Basic Concepts</h3><ul class=""><li class="navListItem"><a class="navItem" href="/v/0.1.0/docs/overview">Overview</a></li><li class="navListItem"><a class="navItem" href="/v/0.1.0/docs/models">Models</a></li><li class="navListItem"><a class="navItem" href="/v/0.1.0/docs/posteriors">Posteriors</a></li><li class="navListItem navListItemActive"><a class="navItem" href="/v/0.1.0/docs/acquisition">Acquisition Functions</a></li><li class="navListItem"><a class="navItem" href="/v/0.1.0/docs/optimization">Optimization</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Advanced Topics</h3><ul class=""><li class="navListItem"><a class="navItem" href="/v/0.1.0/docs/batching">Batching</a></li><li class="navListItem"><a class="navItem" href="/v/0.1.0/docs/objectives">Objectives</a></li><li class="navListItem"><a class="navItem" href="/v/0.1.0/docs/samplers">Monte Carlo Samplers</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer"><div class="wrapper"><div class="post"><header class="postHeader"><a class="edit-page-link button" href="https://github.com/pytorch/botorch/edit/master/docs/acquisition.md" target="_blank" rel="noreferrer noopener">Edit</a><h1 class="postHeaderTitle">Acquisition Functions</h1></header><article><div><span><p>Acquisition functions are heuristics employed to evaluate the usefulness of one
of more design points for achieving the objective of maximizing the underlying
black box function.</p>
<p>BoTorch supports both analytic as well as (quasi-) Monte-Carlo based acquisition
functions. It provides a generic
<a href="../api/acquisition.html#acquisitionfunction"><code>AcquisitionFunction</code></a> API that
abstracts away from the particular type, so that optimization can be performed
on the same objects.</p>
<h2><a class="anchor" aria-hidden="true" id="monte-carlo-acquisition-functions"></a><a href="#monte-carlo-acquisition-functions" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Monte Carlo Acquisition Functions</h2>
<p>Many common acquisition functions can be expressed as the expectation of some
real-valued function of the model output(s) at the design point(s):</p>
<p>$$
\alpha(X) = \mathbb{E}\bigl[ a(\xi) \mid
\xi \sim \mathbb{P}(f(X) \mid \mathcal{D}) \bigr]
$$</p>
<p>where $X = (x_1, \dotsc, x_q)$, and $\mathbb{P}(f(X) \mid \mathcal{D})$ is the
posterior distribution of the function $f$ at $X$ given the data $\mathcal{D}$
observed so far.</p>
<p>Evaluating the acquisition function thus requires evaluating an integral over
the posterior distribution. In most cases, this is analytically intractable. In
particular, analytic expressions generally do not exist for batch acquisition
functions that consider multiple design points jointly (i.e. $q &gt; 1$).</p>
<p>An alternative is to use Monte-Carlo (MC) sampling to approximate the integrals.
An MC approximation of $\alpha$ at $X$ using $N$ MC samples is</p>
<p>$$ \alpha(X) \approx \frac{1}{N} \sum_{i=1}^N a(\xi_{i}) $$</p>
<p>where $\xi_i \sim \mathbb{P}(f(X) \mid \mathcal{D})$.</p>
<p>For instance, for q-Expected Improvement (qEI), we have:</p>
<p>$$
\text{qEI}(X) \approx \frac{1}{N} \sum_{i=1}^N \max_{j=1,..., q}
\bigl\{ \max(\xi_{ij} - f^*, 0) \bigr\},
\qquad \xi_{i} \sim \mathbb{P}(f(X) \mid \mathcal{D})
$$</p>
<p>where $f^*$ is the best function value observed so far (assuming noiseless
observations). Using the reparameterization trick (<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>,
<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>),</p>
<p>$$
\text{qEI}(X) \approx \frac{1}{N} \sum_{i=1}^N \max_{j=1,..., q}
\bigl\{ \max\bigl( \mu(X)_j + (L(X) \epsilon_i)_j - f^*, 0 \bigr) \bigr\},
\qquad \epsilon_{i} \sim \mathcal{N}(0, I)
$$</p>
<p>where $\mu(X)$ is the posterior mean of $f$ at $X$, and $L(X)L(X)^T = \Sigma(X)$
is a root decomposition of the posterior covariance matrix.</p>
<p>All MC-based acquisition functions in BoTorch are derived from
<a href="../api/acquisition.html#mcacquisitionfunction"><code>MCAcquisitionFunction</code></a>.</p>
<p>Acquisition functions expect input tensors $X$ of shape
$\textit{batch_shape} \times q \times d$, where $d$ is the dimension of the
feature space, $q$ is the number of points considered jointly, and
$\textit{batch_shape}$ is the batch-shape of the input tensor. The output
$\alpha(X)$ will have shape $\textit{batch_shape}$, with each element
corresponding to the respective $q \times d$ batch tensor in the input $X$.
Note that for analytic acquisition functions, it must be that $q=1$.</p>
<h3><a class="anchor" aria-hidden="true" id="mc-q-mc-and-fixed-base-samples"></a><a href="#mc-q-mc-and-fixed-base-samples" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>MC, q-MC, and Fixed Base Samples</h3>
<p>BoTorch relies on the re-parameterization trick and (quasi)-Monte-Carlo sampling
for optimization and estimation of the batch acquisition functions <sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>.
The results below show the reduced variance when estimating an expected
improvement (EI) acquisition function using base samples obtained via quasi-MC
sampling versus standard MC sampling.</p>
<p><img src="/v/0.1.0/docs/assets/EI_MC_qMC.png" alt="MC_qMC"></p>
<p>In the plots above, the base samples used to estimate each point are resampled.
As discussed in the <a href="./overview">Overview</a>, a single set of base samples can be
used for optimization when the re-parameterization trick is employed. What are the
trade-offs between using a fixed set of base samples versus re-sampling on every
MC evaluation of the acquisition function? Below, we show that fixing base samples
produces functions that are potentially much easier to optimize, without resorting to
stochastic optimization methods.</p>
<p><img src="/v/0.1.0/docs/assets/EI_resampling_fixed.png" alt="resampling_fixed"></p>
<p>If the base samples are fixed, the problem of optimizing the acquisition function
is deterministic, allowing for conventional quasi-second order methods such as
L-BFGS or sequential least-squares programming (SLSQP) to be used. These have
faster convergence rates than first-order methods and can speed up acquisition
function optimization significantly.</p>
<p>One concern is that the approximated acquisition function is <em>biased</em> for any
fixed set of base samples, which may adversely affect the solution. However, we
find that in practice, both the optimal value and the optimal solution of these
biased problems for standard acquisition functions converge quite rapidly to
their true counterparts as more samples are used. Note that for evaluation of
the acquisition function we integrate over a $qo$-dimensional space (where
$q$ is the number of points in the q-batch and $o$ is the number of outputs
included in the objective). Therefore, the MC integration problem can be quite
low-dimensional even for models on high-dimensional feature spaces (large $d$).
Because using additional samples is relatively cheap computationally,
we default to 500 base samples in the MC acquisition functions.</p>
<p>On the other hand, when re-sampling is used in conjunction with a stochastic
optimization algorithm, the kind of bias noted above is no longer a concern.
The trade-off here is that the optimization may be less effective, as discussed
above.</p>
<h2><a class="anchor" aria-hidden="true" id="analytic-acquisition-functions"></a><a href="#analytic-acquisition-functions" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Analytic Acquisition Functions</h2>
<p>BoTorch also provides implementations of analytic acquisition functions that
do not depend on MC sampling. These acquisition functions are subclasses of
<a href="../api/acquisition.html#analyticacquisitionfunction"><code>AnalyticAcquisitionFunction</code></a>
and only exist for the case of a single candidate point ($q = 1$). These
include classical acquisition functions such as Expected Improvement (EI),
Upper Confidence Bound (UCB), and Probability of Improvement (PI). An example
comparing <a href="../api/acquisition.html#expectedimprovement"><code>ExpectedImprovement</code></a>,
the analytic version of EI, to it's MC counterpart
<a href="../api/acquisition.html#qexpectedimprovement"><code>qExpectedImprovement</code></a>
can be found in
<a href="../tutorials/compare_mc_analytic_acquisition">this tutorial</a>.</p>
<p>Analytic acquisition functions allow for an explicit expression in terms of the
summary statistics of the posterior distribution at the evaluated point(s).
A popular acquisition function is Expected Improvement of a single point
for a Gaussian posterior, given by</p>
<p>$$ \text{EI}(x) = \mathbb{E}\bigl[
\max(y - f^*, 0) \mid y\sim \mathcal{N}(\mu(x), \sigma^2(x))
\bigr] $$</p>
<p>where $\mu(x)$ and $\sigma(x)$ are the posterior mean and variance of $f$ at the
point $x$, and $f^*$ is again the best function value observed so far (assuming
noiseless observations). It can be shown that</p>
<p>$$ \text{EI}(x) = \sigma(x) \bigl( z \Phi(z) + \varphi(z) \bigr)$$</p>
<p>where $z = \frac{\mu(x) - f_{\max}}{\sigma(x)}$ and $\Phi$ and $\varphi$ are
the cdf and pdf of the standard normal distribution, respectively.</p>
<p>With some additional work, it is also possible to express the gradient of
the Expected Improvement with respect to the design $x$. Classic Bayesian
Optimization software will implement this gradient function explicitly, so that
it can be used for numerically optimizing the acquisition function.</p>
<p>BoTorch, in contrast, harnesses PyTorch's automatic differentiation feature
(&quot;autograd&quot;) in order to obtain gradients of acquisition functions. This makes
implementing new acquisition functions much less cumbersome, as it does not
require to analytically derive gradients. All that is required is that the
operations performed in the acquisition function computation allow for the
back-propagation of gradient information through the posterior and the model.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1"  class="footnote-item"><p>D. P. Kingma, M. Welling. Auto-Encoding Variational Bayes.
ICLR, 2013. <a href="#fnref1" class="footnote-backref">↩</a></p>
</li>
<li id="fn2"  class="footnote-item"><p>D. J. Rezende, S. Mohamed, D. Wierstra. Stochastic
Backpropagation and Approximate Inference in Deep Generative Models. ICML, 2014. <a href="#fnref2" class="footnote-backref">↩</a></p>
</li>
<li id="fn3"  class="footnote-item"><p>J. T. Wilson, R. Moriconi, F. Hutter, M. P. Deisenroth.
The Reparameterization Trick for Acquisition Functions. NeurIPS Workshop on
Bayesian Optimization, 2017. <a href="#fnref3" class="footnote-backref">↩</a></p>
</li>
</ol>
</section>
</span></div></article></div><div class="docs-prevnext"><a class="docs-prev button" href="/v/0.1.0/docs/posteriors"><span class="arrow-prev">← </span><span>Posteriors</span></a><a class="docs-next button" href="/v/0.1.0/docs/optimization"><span>Optimization</span><span class="arrow-next"> →</span></a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#monte-carlo-acquisition-functions">Monte Carlo Acquisition Functions</a><ul class="toc-headings"><li><a href="#mc-q-mc-and-fixed-base-samples">MC, q-MC, and Fixed Base Samples</a></li></ul></li><li><a href="#analytic-acquisition-functions">Analytic Acquisition Functions</a></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/v/0.1.0/" class="nav-home"><img src="/v/0.1.0/img/botorch.png" alt="BoTorch" width="66" height="58"/></a><div class="footerSection"><h5>Docs</h5><a href="/v/0.1.0/docs/introduction">Introduction</a><a href="/v/0.1.0/docs/getting_started">Getting Started</a><a href="/v/0.1.0/tutorials">Tutorials</a><a href="/v/0.1.0/api">API Reference</a></div><div class="footerSection"><h5>Social</h5><div class="social"><a class="github-button" href="https://github.com/pytorch/botorch" data-count-href="https://github.com/pytorch/botorch/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star BoTorch on GitHub">botorch</a></div></div></section><a href="https://opensource.facebook.com/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/v/0.1.0/img/oss_logo.png" alt="Facebook Open Source" width="170" height="45"/></a><section class="copyright"> Copyright © 2019 Facebook Inc.</section></footer></div></body></html>